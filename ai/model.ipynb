{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c5870336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "5b257fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA INGESTION\n",
    "\n",
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, csv_dir, labels_dict, statistical_processing=False):\n",
    "        self.csv_dir = csv_dir\n",
    "        self.labels_dict = labels_dict\n",
    "        self.file_list = [f for f in os.listdir(csv_dir) if f in self.labels_dict]\n",
    "        self.statistical_processing = statistical_processing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        file_path = os.path.join(self.csv_dir, file_name)\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        features = df.iloc[:, 1:].values\n",
    "        if self.statistical_processing:\n",
    "            # Process for each feature -> x3\n",
    "            mean_cols = np.tile(np.mean(features, axis=0), (25, 1))\n",
    "            std_cols = np.tile(np.std(features, axis=0), (25, 1))\n",
    "            features = np.hstack([features, mean_cols, std_cols])\n",
    "            \n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels_dict[file_name], dtype=torch.float32)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "def create_dataloader(dataset, csv_dir, labels_dict, statistical_processing=False, batch_size=4):\n",
    "    data = dataset(csv_dir, labels_dict, statistical_processing)\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "519743d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODELS\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# 1D CNN -> comparable to RNN with less overhead\n",
    "class CNN1DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNN1DClassifier, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv_block(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "# 3-layer MLP -> likely worst\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dropout_rate=0.2):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "d7011310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 2.0907\n",
      "Epoch 2/20 - Loss: 2.0487\n",
      "Epoch 3/20 - Loss: 2.0122\n",
      "Epoch 4/20 - Loss: 1.9810\n",
      "Epoch 5/20 - Loss: 1.9495\n",
      "Epoch 6/20 - Loss: 1.9165\n",
      "Epoch 7/20 - Loss: 1.8821\n",
      "Epoch 8/20 - Loss: 1.8477\n",
      "Epoch 9/20 - Loss: 1.8133\n",
      "Epoch 10/20 - Loss: 1.7773\n",
      "Epoch 11/20 - Loss: 1.7405\n",
      "Epoch 12/20 - Loss: 1.7021\n",
      "Epoch 13/20 - Loss: 1.6599\n",
      "Epoch 14/20 - Loss: 1.6166\n",
      "Epoch 15/20 - Loss: 1.5738\n",
      "Epoch 16/20 - Loss: 1.5313\n",
      "Epoch 17/20 - Loss: 1.4894\n",
      "Epoch 18/20 - Loss: 1.4495\n",
      "Epoch 19/20 - Loss: 1.4118\n",
      "Epoch 20/20 - Loss: 1.3766\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "def train_model(dataloader, model, criterion, optimiser, epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_features, batch_labels in dataloader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_labels = batch_labels.to(device).long()\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # return loss.item()         # For Optuna, eventually\n",
    "\n",
    "statistical_processing = True\n",
    "data_folder_path = \"data/dummy/dataset\"\n",
    "with open(\"data/dummy/dataset/labels.json\", \"r\") as f:\n",
    "    labels_dict = json.load(f)\n",
    "dataloader = create_dataloader(SensorDataset, data_folder_path, labels_dict, statistical_processing, batch_size=5)\n",
    "\n",
    "model = LSTMClassifier(input_size=8*3 if statistical_processing else 8, hidden_size=64, num_layers=2, num_classes=8)\n",
    "# model = MLPClassifier(input_size=8, num_classes=8)\n",
    "# model = CNN1DClassifier(input_size=8, num_classes=8)\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "train_model(dataloader, model, criterion, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "eaef8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTION AND EVALUATION\n",
    "\n",
    "def get_features_from_csv(file_path, statistical_processing=False):\n",
    "    df = pd.read_csv(file_path)\n",
    "    features = df.iloc[:, 1:].values\n",
    "\n",
    "    if statistical_processing:\n",
    "        mean_cols = np.tile(np.mean(features, axis=0), (25, 1))\n",
    "        std_cols = np.tile(np.std(features, axis=0), (25, 1))\n",
    "        features = np.hstack([features, mean_cols, std_cols])\n",
    "\n",
    "    return features\n",
    "\n",
    "def predict_csv(model, features):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    input_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        pred_class = torch.argmax(probs, dim=1).item()\n",
    "        confidence = torch.max(probs).item()\n",
    "        \n",
    "    return pred_class, confidence\n",
    "\n",
    "def evaluate_folder(model, folder_path, labels_dict, statistical_processing=False):       \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for file_name, label in labels_dict.items():\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        features = get_features_from_csv(file_path, statistical_processing)\n",
    "\n",
    "        pred, confidence = predict_csv(model, features)\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=list(range(8)))\n",
    "    report = classification_report(y_true, y_pred, labels=list(range(8)), zero_division=0)\n",
    "    \n",
    "    return accuracy, conf_matrix, report\n",
    "\n",
    "acc, cm, report = evaluate_folder(model, data_folder_path, labels_dict, statistical_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6d2c6",
   "metadata": {},
   "source": [
    "### Exploration with Kaggle data\n",
    "\n",
    "Aim is to validate model choice, and that entire pipeline from data ingestion to prediction works.\n",
    "\n",
    "Data from: https://www.kaggle.com/datasets/harrisonlou/imu-glove/data <br>\n",
    "Note that data files are not standardised, so I had to do windowing.\n",
    "\n",
    "#### Results\n",
    "\n",
    "| Model | Accuracy / % |\n",
    "|---|---|\n",
    "| RNN + LSTM (with mean, std dev) | 100 |\n",
    "| RNN + LSTM | 100 |\n",
    "| 1D CNN | 99.2 |\n",
    "| MLP| 99.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9627bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedCSVDataset(Dataset):\n",
    "    def __init__(self, csv_dir, labels_dict, statistical_processing=False, window_size=25, stride=5):\n",
    "        self.samples = []\n",
    "      \n",
    "        for file_name, label in labels_dict.items():\n",
    "            file_path = os.path.join(csv_dir, file_name)\n",
    "            if not os.path.exists(file_path):\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "            x = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32)  # [T, F]\n",
    "\n",
    "            T, F = x.shape\n",
    "\n",
    "            # handle short CSVs\n",
    "            if T < window_size:\n",
    "                pad = torch.zeros(window_size - T, F)\n",
    "                window = torch.cat([x, pad], dim=0)  # [window_size, F]\n",
    "                self.samples.append((window, torch.tensor(label, dtype=torch.long)))\n",
    "\n",
    "            for start in range(0, T - window_size + 1, stride):\n",
    "                window = x[start:start + window_size] # [25, 44]\n",
    "                \n",
    "                if statistical_processing:\n",
    "                    # Calculate stats for this specific window\n",
    "                    w_mean = window.mean(dim=0) # [44]\n",
    "                    w_std = window.std(dim=0)   # [44]\n",
    "                    \n",
    "                    # Expand stats to match window length [25, 44]\n",
    "                    mean_feat = w_mean.unsqueeze(0).expand(window_size, -1)\n",
    "                    std_feat = w_std.unsqueeze(0).expand(window_size, -1)\n",
    "                    \n",
    "                    # Concat: [25, 44 + 44 + 44] -> [25, 132]\n",
    "                    window = torch.cat([window, mean_feat, std_feat], dim=-1)\n",
    "                \n",
    "                self.samples.append((window, torch.tensor(label, dtype=torch.long)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "932d74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_data_path = \"data/kaggle_imu/data\"\n",
    "imu_label_path = \"data/kaggle_imu/label\"        # labels are [0, 1, 2, 3, 4, 5, 6, 10->7]\n",
    "\n",
    "def get_labels(label_folder_path):\n",
    "    labels = dict()\n",
    "\n",
    "    for file_name in os.listdir(label_folder_path):\n",
    "        file_path = os.path.join(label_folder_path, file_name)\n",
    "        data = pd.read_csv(file_path)\n",
    "        label = data.iat[0,0]\n",
    "        file_name = file_name.replace(\"label\", \"data\")\n",
    "        labels[file_name] = label if label != 10 else 7\n",
    "\n",
    "    return labels\n",
    "\n",
    "labels_kaggle = get_labels(imu_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4cd8b59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.5949\n",
      "Epoch 2/20 - Loss: 0.2875\n",
      "Epoch 3/20 - Loss: 0.1718\n",
      "Epoch 4/20 - Loss: 0.1238\n",
      "Epoch 5/20 - Loss: 0.0898\n",
      "Epoch 6/20 - Loss: 0.0700\n",
      "Epoch 7/20 - Loss: 0.0535\n",
      "Epoch 8/20 - Loss: 0.0530\n",
      "Epoch 9/20 - Loss: 0.0432\n",
      "Epoch 10/20 - Loss: 0.0363\n",
      "Epoch 11/20 - Loss: 0.0389\n",
      "Epoch 12/20 - Loss: 0.0277\n",
      "Epoch 13/20 - Loss: 0.0300\n",
      "Epoch 14/20 - Loss: 0.0280\n",
      "Epoch 15/20 - Loss: 0.0211\n",
      "Epoch 16/20 - Loss: 0.0257\n",
      "Epoch 17/20 - Loss: 0.0221\n",
      "Epoch 18/20 - Loss: 0.0199\n",
      "Epoch 19/20 - Loss: 0.0178\n",
      "Epoch 20/20 - Loss: 0.0189\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "statistical_processing = True\n",
    "dataloader = create_dataloader(WindowedCSVDataset, imu_data_path, labels_kaggle, statistical_processing, batch_size=5)\n",
    "\n",
    "model_kaggle = LSTMClassifier(input_size=44*3 if statistical_processing else 44, hidden_size=64, num_layers=2, num_classes=8)\n",
    "# model_kaggle = MLPClassifier(input_size=25*44, num_classes=8)     # window_size * features\n",
    "# model_kaggle = CNN1DClassifier(input_size=44, num_classes=8)\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model_kaggle.parameters(), lr)\n",
    "\n",
    "train_model(dataloader, model_kaggle, criterion, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTION AND EVALUATION\n",
    "\n",
    "def get_windows_from_csv(file_path, window_size=25, stride=5, statistical_processing=False):\n",
    "    df = pd.read_csv(file_path)\n",
    "    x = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32)  # [T, F]\n",
    "    T, F = x.shape\n",
    "\n",
    "    if T < window_size:\n",
    "        pad = torch.zeros(window_size - T, F)\n",
    "        x = torch.cat([x, pad], dim=0)\n",
    "        T = window_size\n",
    "\n",
    "    windows = []\n",
    "    for start in range(0, T - window_size + 1, stride):\n",
    "        window = x[start:start + window_size]  # [25, 44]\n",
    "        \n",
    "        # Add Stats (Must match the Training Dataset logic exactly!)\n",
    "        if statistical_processing:\n",
    "            w_mean = window.mean(dim=0).unsqueeze(0).expand(window_size, -1)\n",
    "            w_std = window.std(dim=0).unsqueeze(0).expand(window_size, -1)\n",
    "            window = torch.cat([window, w_mean, w_std], dim=-1) # [25, 132]\n",
    "            \n",
    "        windows.append(window)\n",
    "\n",
    "    return torch.stack(windows)  # [num_windows, 25, 132 or 44]\n",
    "\n",
    "def predict_csv(model, windows):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    windows = windows.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(windows)             # [num_windows, num_classes]\n",
    "        probs = torch.softmax(logits, dim=1) # [num_windows, num_classes]\n",
    "\n",
    "        # Aggregate: mean across windows\n",
    "        avg_probs = probs.mean(dim=0)        # [num_classes]\n",
    "        pred_class = torch.argmax(avg_probs).item()\n",
    "        confidence = torch.max(avg_probs).item()\n",
    "\n",
    "    return pred_class, confidence\n",
    "\n",
    "def evaluate_folder(model, folder_path, labels_dict, window_size=25, stride=5, statistical_processing=False):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for file_name, label in labels_dict.items():\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        windows = get_windows_from_csv(file_path, window_size, stride, statistical_processing)\n",
    "\n",
    "        pred, confidence = predict_csv(model, windows)\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=list(range(8)))\n",
    "    report = classification_report(y_true, y_pred, labels=list(range(8)), zero_division=0)\n",
    "    \n",
    "    return accuracy, conf_matrix, report\n",
    "\n",
    "acc, cm, report = evaluate_folder(model_kaggle, imu_data_path, labels_kaggle, statistical_processing=statistical_processing)\n",
    "# RNN + LSTM (with mean, std dev): 1\n",
    "# RNN + LSTM: 1\n",
    "# 1D CNN: 0.992\n",
    "# MLP: 0.990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "63b5f329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
